{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Timing Part 2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkSxvGeTDl/HTkV+NcTHJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamiresco/Spike-Timing/blob/master/Timing_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxy5Chlxzreb",
        "colab_type": "text"
      },
      "source": [
        "###Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR6CvBy_1-kQ",
        "colab_type": "text"
      },
      "source": [
        "* Data Frame 3: Propriedades das distribuiçoes (entropia, entropia conjunta, etc.)\n",
        "* Feature Selection: teste de hipotese em cada feature\n",
        "* Modelo: Com os atributos das distribuições da sessao 1 e 2(?) preveremos quando a porcentagem de acerto dos ratos na sessao 7(?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo7eMb_X2Yga",
        "colab_type": "text"
      },
      "source": [
        "Envirolment setting up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk-3pZpUz8xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# basicos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# import dask as dask\n",
        "from scipy import stats\n",
        "\n",
        "# google colab\n",
        "from google.colab import files\n",
        "\n",
        "# feature selection\n",
        "!pip install boruta\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from boruta import BorutaPy\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# model selection\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# model\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDVg_LhW1WUl",
        "colab_type": "text"
      },
      "source": [
        "Medidas da Teoria da Informação que podem ser usadas\n",
        "\n",
        "\n",
        "1. **[Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))** is a basic quantity in information theory associated to any random variable, which can be interpreted as the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes.\n",
        "2. **[Joint entropy](https://en.wikipedia.org/wiki/Joint_entropy)** is a measure of the uncertainty associated with a set of variables.\n",
        "3. **[Conditional entropy](https://en.wikipedia.org/wiki/Conditional_entropy)** quantifies the amount of information needed to describe the outcome of a random variable Y given that the value of another random variable X is known. Here, information is measured in shannons, nats, or hartleys. The entropy of Y conditioned on X is written as H(Y|X).\n",
        "4. **[Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)** between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\n",
        "5. **[Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence)** (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution.\n",
        "6. **Symmetrised Kullback-Leibler divergence**\n",
        "7. **[Jensen-Shannon divergence](https://en.wikipedia.org/wiki/Jensen–Shannon_divergence)** is a method of measuring the similarity between two probability distributions.\n",
        "8. **[Mutual information](https://en.wikipedia.org/wiki/Mutual_information)** of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the \"amount of information\" (in units such as shannons, commonly called bits) obtained about one random variable through observing the other random variable. \n",
        "9. **Normalised mutual information** (7 variants)\n",
        "10. **[Variation of information](https://en.wikipedia.org/wiki/Variation_of_information)**  the variation of information or shared information distance is a measure of the distance between two clusterings (partitions of elements).\n",
        "11. **Lautum information**\n",
        "12. **[Conditional mutual information](https://en.wikipedia.org/wiki/Conditional_mutual_information)** is, in its most basic form, the expected value of the mutual information of two random variables given the value of a third.\n",
        "13. **[Interaction Information](https://en.wikipedia.org/wiki/Interaction_information)** is a generalization of mutual information, but unlike the mutual information, the interaction information can be either positive or negative. \n",
        "14. **Co-information** is the same as Interaction Information by wikipedia.\n",
        "15. **[Multi-information](https://en.wikipedia.org/wiki/Total_correlation)** or total correlation (Watanabe 1960) is one of several generalizations of the mutual information.\n",
        "16. **Binding information**\n",
        "17. **[Residual entropy](https://en.wikipedia.org/wiki/Residual_entropy)** is the difference in entropy between a non-equilibrium state and crystal state of a substance close to absolute zero. \n",
        "18. **Exogenous local information**\n",
        "19. **Enigmatic information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ejV-KhhzxuH",
        "colab_type": "text"
      },
      "source": [
        "###Funções\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fJ4Cwuj0bhL",
        "colab_type": "text"
      },
      "source": [
        "Feature Engenering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TItdqA911MUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_columns_mean(dataFrame2): \n",
        "\n",
        "  df_mean = dataFrame2.loc[:,['T', 'A', 'E']].applymap(lambda x: np.mean(x)).groupby(level=0).mean()\n",
        "  df_mean.rename({'T': 'T_mean', 'A' : 'A_mean', 'E': 'E_mean'}, axis=1, inplace=True)\n",
        " \n",
        "  df_std = dataFrame2.loc[:,['T', 'A', 'E']].applymap(lambda x: np.std(x)).groupby(level=0).mean() \n",
        "  df_std.rename({'T': 'T_std', 'A' : 'A_std', 'E': 'E_std'}, axis=1, inplace=True)\n",
        "  \n",
        "  df_kurtosis = dataFrame2.loc[:,['T', 'A', 'E']].applymap(lambda x: stats.kurtosis(x)).groupby(level=0).mean()\n",
        "  df_kurtosis.rename({'T': 'T_kurtosis', 'A' : 'A_kurtosis', 'E': 'E_kurtosis'}, axis=1, inplace=True)\n",
        "\n",
        "  dataFrame3 = pd.concat([df_mean, df_std, df_kurtosis], axis =1)\n",
        "\n",
        "  return dataFrame3\n",
        "\n",
        "def bays(likelihood, prior, posteriorTrue, tmax = 5):  # ou tmax = max(likelihood)\n",
        "\n",
        "  # alisando as 3 curvas de distribuicao e colocando como maximo o tmax\n",
        "  grid1, likelihood = kernelDensityEstimation (likelihood, dt=0.1, bw_method = 'scott', tmax=tmax)\n",
        "  grid2, prior = kernelDensityEstimation (prior, dt=0.1, bw_method = 'scott', tmax=tmax)\n",
        "  grid3, posteriorTrue = kernelDensityEstimation (posteriorTrue, dt=0.1, bw_method = 'scott', tmax=tmax)\n",
        "\n",
        "  # formula de probalibilidade condicional\n",
        "  posteriorEstimation = [prior[i] * likelihood[i] for i in range(len(likelihood))] \n",
        "\n",
        "  # comparacao distribuicao estimada por update baysiano e distribuicao real\n",
        "  score = mean_absolute_error(posteriorEstimation, posteriorTrue)\n",
        "\n",
        "  return (score) \n",
        "\n",
        "def add_columns_bays_error(dataFrame2, dataFrame3):\n",
        "\n",
        "  # preparacao\n",
        "  df = dataFrame2.drop(['ITI', 'B', 'C', 'D', 'F', 'G', 'H', 'I', 'J', 'success', 'fail', 'rate success' ], axis=1)\n",
        "  score = []\n",
        "  mean_score = []\n",
        "\n",
        "  for j in df.columns:  \n",
        "    for i in dataFrame3.index:\n",
        "\n",
        "      # aplicando a funcao q compara a distribuicao posterior esperada com a real em 2 conjuntos\n",
        "      score.append( bays(df.at[(i,1),j], df.at[(i,2),j], df.at[(i,3),j]))\n",
        "      score.append( bays(df.at[(i,2),j], df.at[(i,3),j], df.at[(i,6),j]))\n",
        "\n",
        "      # depois tira-se a media do erro dos dois conjuntos\n",
        "      mean_score.append(np.mean(score))\n",
        "\n",
        "      # esvazia para fazer o calculo para o proximo rato\n",
        "      score = []\n",
        "\n",
        "  # fragmentar o vetor por distribuição (cada distribuicao tem nRats q estao empilhalhos)\n",
        "  nRats = len(dataFrame3.index) \n",
        "  dataFrame3['T_bays'] = mean_score[0 : nRats]\n",
        "  dataFrame3['A_bays'] = mean_score[nRats : 2*nRats]\n",
        "  dataFrame3['E_bays'] = mean_score[2*nRats : 3*nRats]\n",
        "\n",
        "  return dataFrame3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTzp8zbu0I88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kernelDensityEstimation (df, dt, bw_method, tmax = 0): \n",
        "\n",
        "  tmin = 0\n",
        "  tmax = max(df) if tmax == 0 else tmax\n",
        "\n",
        "  kde = stats.gaussian_kde(df, bw_method=bw_method ) # instanciar gaussian_kde\n",
        "  counts = int((tmax - tmin) / dt) # criar intervalo com mesma quantidade de canais que o histograma  \n",
        "  grid = np.linspace(tmin, tmax, counts) #  continuando a criação\n",
        "  pdf = kde.evaluate(grid)\n",
        "  \n",
        "  return grid, pdf\n",
        "\n",
        "def differentialEntropy(p, dt, unidade = 'digits'):\n",
        "  \n",
        "  # multiplicação por dt garante a continuidade da variavel\n",
        "  if unidade == 'nats':\n",
        "    h = -np.sum(p*np.log(p))*dt \n",
        "  elif unidade == 'digits':\n",
        "    h = -np.sum(p*np.log10(p))*dt\n",
        "  elif unidade == 'bits':\n",
        "    h = -np.sum(p*np.log2(p))*dt\n",
        "\n",
        "  return h\n",
        "\n",
        "def entropy (df, dt = 0.1, bw_method = 'scott'): # bw é o parametro bandwidth que determina o quão suavisada sera a curva \n",
        "\n",
        "  grid, pdf = kernelDensityEstimation(df, dt, bw_method) # estimar densidade probabilistica com kernel \n",
        "  h_kde = differentialEntropy(pdf, dt)  # calcular sua entropia diferencial\n",
        "  \n",
        "  return np.round(h_kde,2)\n",
        "\n",
        "def add_columns_entropy(dataFrame2, dataFrame3): #TO DO\n",
        "\n",
        "  df = dataFrame2.drop([ 'success', 'fail', 'rate success' ], axis=1)\n",
        "  df = df.applymap(lambda x: entropy(x)).groupby(level=0).mean()\n",
        "  df.rename({'T': 'T_entropy', 'ITI': 'ITI_entropy', 'A' : 'A_entropy', 'B': 'B_entropy', 'C': 'C_entropy', 'D': 'D_entropy', 'E': 'E_entrpy', 'F': 'F_entropy', 'G': 'G_entropy', 'H': 'H_entropy', 'I': 'I_entropy', 'J': 'J_entropy'}, axis=1, inplace=True)\n",
        "  dataFrame3 = pd.concat([dataFrame3, df], axis =1)\n",
        "\n",
        "  return dataFrame3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gas_b7d0EjT",
        "colab_type": "text"
      },
      "source": [
        "Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auvTHAltz9YA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing (dataFrame2, dataFrame3): \n",
        "\n",
        "  X = dataFrame3.applymap(lambda x: 100*round(x,2)).astype('int32') #multiplicando por 100, arredondando e definindo como inteiro: 0.453 = 45 \n",
        "  X = np.array(X) #transformando em np array pq o boruta pede isso\n",
        "\n",
        "  sessions_drop = range(1, len(sessions)) # definindo q todas colunas menos a ultima serao eliminadas, assim a ultima sera a q o success rate vira o target\n",
        "  y = dataFrame2.drop(columns=['T','ITI','A', 'B', 'C', 'D','E', 'F', 'G', 'H', 'I', 'J', 'success', 'fail']) # ficar so com o success rate\n",
        "  y = y.apply(lambda x: 100*round(x,2)).astype('int32').drop(index=sessions_drop, level=1) \n",
        "\n",
        "  return X, y\n",
        "  \n",
        "def boruta (dataFrame2, dataFrame3):\n",
        "\n",
        "  X, y = preprocessing(dataFrame2, dataFrame3)\n",
        "\n",
        "  #iniciando funcoes \n",
        "  rfc = RandomForestClassifier(n_estimators=200, n_jobs=-1, class_weight= 'balanced_subsample' , max_depth=10) \n",
        "  boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1, max_iter = 50, perc = 90) #TO DO entender melhor \n",
        "\n",
        "  #aplicando-as aos dados\n",
        "  boruta_selector.fit(X, y)\n",
        "\n",
        "  #colunas q passaram no teste de hipotese\n",
        "  if (boruta.support_) != 0 :\n",
        "    cols_support = X.columns[boruta.support_]\n",
        "  if (boruta.upport_weak_) != 0 :\n",
        "    cols_support_weak_ = X.columns[boruta.upport_weak_]\n",
        "\n",
        "    return X[cols_support], X[cols_support_weak_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcRV8cuz4B5",
        "colab_type": "text"
      },
      "source": [
        "### Rodando"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoOGCOWu0RJt",
        "colab_type": "text"
      },
      "source": [
        "Feature Engenering\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAAim1-yzdIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lendo csv\n",
        "dataFrame2 = pd.read_csv('dataFrame2_9.csv')\n",
        "\n",
        "# arrumando index\n",
        "dataFrame2.set_index(['rat', 'session'], inplace=True)\n",
        "\n",
        "# pegando propriedades das distribuições\n",
        "dataFrame3 = add_columns_mean(dataFrame2)\n",
        "dataFrame3 = add_columns_bays_error(dataFrame2, dataFrame3)\n",
        "dataFrame3 = add_columns_entropy(dataFrame2, dataFrame3)\n",
        "\n",
        "# df.applymap(lambda x: max(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcnU2CjS0MYW",
        "colab_type": "text"
      },
      "source": [
        "Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG-tNAYe0PqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boruta (dataFrame2, dataFrame3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3S4ars30hGp",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdL9QOUz0lgM",
        "colab_type": "text"
      },
      "source": [
        "Treino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nhqzDTQ0lNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separando os dados de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
        "\n",
        "# treinando regressor\n",
        "xgbr = xgb.XGBRegressor(verbosity=0)  # (?) (n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
        "xgbr.fit(X_train, y_train) # (?) (X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False) \n",
        "score = xgbr.score(xtrain, ytrain)   \n",
        "print(\"Training score: \", score) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDmtgLv10k5p",
        "colab_type": "text"
      },
      "source": [
        "Validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3VAckQS0kmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cross validataion com cv = 5 e cv = Kfold (para comparar)\n",
        "scores = cross_val_score(X_train, y_train, cv=5)\n",
        "print(\"Mean cross-validation score: %.2f\" % scores.mean())\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "kf_cv_scores = cross_val_score(xgbr, X_train, y_train, cv=kfold )\n",
        "print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqmsPdla0kUD",
        "colab_type": "text"
      },
      "source": [
        "Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT4_v-MP0j5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testando regressor\n",
        "y_pred = xgbr.predict(X_test) # (?) nao deveria fazer com um X_valid ate eu ter certeza q é a ultima alteracao q estou fazendo no modelo\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"r2: %.2f\" % r2)\n",
        "print(\"r2: %.2f\" % (r2*(1/2.0)))\n",
        "\n",
        "# graficos\n",
        "x_ax = range(len(ytest))\n",
        "plt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\n",
        "plt.plot(x_ax, y_pred, lw=0.8, color=\"red\", label=\"predicted\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI_rFOzK0jo1",
        "colab_type": "text"
      },
      "source": [
        "Rascunho"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0MxUTG10w-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # def cross_validation (model, X, y, cv=5, n_jobs=4, scoring='r2'):\n",
        "\n",
        "#   X, X_week = boruta(X, y)\n",
        "#   score = cross_val_score(model, X, y, cv=cv, n_jobs=n_jobs, scoring=scoring)\n",
        "\n",
        "#   print(\"Individuals scores: \", scores())\n",
        "#   print(\"Average scores: \", scores.mean())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}